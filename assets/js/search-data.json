{
  
    
        "post0": {
            "title": "Math and Jupyter Commands",
            "content": "Here in this notebook, we will be looking into &quot;how to write math equations in jupyter notebook&quot;. The reason for writing this notebook is, I want to have all these instructions at once instaed of wrangling around different websites . Lets start with math equation . The math equation cell needs to be in Markdown mode . $ hat{y} = hat{ beta}_{0} + sum limits_{j=1} ^{p} X_{j} hat{ beta}_{j} $ . $ hat{y} = hat{ beta}_{0} + sum limits_{j=1} ^{p} X_{j} hat{ beta}_{j} $ . other math notations . - $ : All the Math you want to write in the markdown should be inside opening and closing $ symbol in order to be processed as Math. - beta : Creates the symbol beta - hat{} : A hat is covered over anything inside the curly braces of hat{}. E.g. in hat{Y} hat is created over Y and in hat{ beta}_{0}, hat is shown over beta - _{} : Creates as subscript, anything inside the curly braces after _. E.g. hat{ beta}_{0} will create beta with a hat and give it a subscript of 0. - ^{} : (Similar to subscript) Creates as superscript, anything inside the curly braces after ^. - sum : Creates the summation symbol - limits _{} ^{} : Creates lower and upper limit for the sum using the subscript and superscript notation. - *** : Creates horizontal line - &emsp; : Creates space. (Ref: Space in ‘markdown’ cell of Jupyter Notebook) - gamma : Creates gamma symbol - displaystyle : Forces display mode (BONUS 3 above). (Ref: Display style in Math mode) - frac{}{} : Creates fraction with two curly braces from numerator and denominator. - &lt;br&gt; : Creates line breaks - Bigg : Helps create parenthesis of big sizes. (Ref: Brackets and Parentheses) - partial : Creates partial derivatives symbol - underset() : To write under a text. E.g. gamma under arg min, instead of a subscript. In the algorithm you’ll see both types. - in : Creates belongs to symbol which is heavily used in set theory. . We can write Math inside two $$ as well. The difference is inline mode vs display mode. &quot;Inline mode is for math that is included within a line or paragraph of text, and display mode is for math that is set apart from the main text.&quot;&quot; . $$f&#39;(a) = lim_{x to a} frac{f(x) - f(a)}{x-a}$$ . $$f&#39;(a) = lim_{x to a} frac{f(x) - f(a)}{x-a}$$ . Latex . Common symbols . from IPython.display import Image Image(filename=&quot;../images/latex/latex_common_symbols_1.png&quot;) . Image(filename=&quot;../images/latex/latex_common_symbols_2.png&quot;) . Matrices and Brackets . Create a matrix without brackets: $$ begin{matrix} a &amp; b c &amp; d end{matrix}$$ . $$ begin{matrix} a &amp; b c &amp; d end{matrix}$$ . Create a matrix with round brackets: $$ begin{pmatrix} a &amp; b c &amp; d end{pmatrix}$$ . $$ begin{pmatrix} a &amp; b c &amp; d end{pmatrix}$$ . Create a matrix with square brackets: $$ begin{bmatrix} 1 &amp; 2 &amp; 1 3 &amp; 0 &amp; 1 0 &amp; 2 &amp; 4 end{bmatrix}$$ . $$ begin{bmatrix} 1 &amp; 2 &amp; 1 3 &amp; 0 &amp; 1 0 &amp; 2 &amp; 4 end{bmatrix}$$ . Use left and right to enclose an arbitrary expression in brackets: $$ left( frac{p}{q} right)$$ . $$ left( frac{p}{q} right)$$ . Jupyter Commands . 1. Text Commands . *emphasis*, **strong**, &#39;code&#39; . emphasis, strong,code . &lt;b&gt;This is bold text &lt;/b&gt; ** This is bold text --This is bold text . This is bold text . **This is bold text . __ This is bold text . 2. Headings . # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 . H1 . H2 . H3 . H4 . H5 . H6 . 3. Lists . 1. Number theory 2. Algebra 3. Partial differential equations 4. Probability . Number theory | Algebra | Partial differential equations | Probability | Create an unordered list using an asterisk * for each item . * Number theory * Algebra * Partial differential equations * Probability . Number theory | Algebra | Partial differential equations | Probability | . - Fish - Eggs - Cheese . Fish | Eggs | Cheese | . &lt;ul&gt; &lt;li&gt;Fish&lt;/li&gt; &lt;li&gt;Eggs&lt;/li&gt; &lt;li&gt;Cheese&lt;/li&gt; &lt;/ul&gt; . Fish | Eggs | Cheese | . Use indentation to create nested lists . 1. Mathematics * Calculus * Linear Algebra * Probability 2. Physics * Classical Mechanics * Relativity * Thermodynamics 3. Biology * Diffusion and Osmosis * Homeostasis * Immunology . Mathematics Calculus | Linear Algebra | Probability | . | Physics Classical Mechanics | Relativity | Thermodynamics | . | Biology Diffusion and Osmosis | Homeostasis | Immunology | . | 4. Tables . | Python Operator | Description | | :: | :: | | `+` | addition | | `-` | subtraction | | `*` | multiplication | | `/` | division | | `**` | power | . Python Operator Description . + | addition | . - | subtraction | . * | multiplication | . / | division | . ** | power | . References . https://medium.com/analytics-vidhya/writing-math-equations-in-jupyter-notebook-a-naive-introduction-a5ce87b9a214 | https://www.math.ubc.ca/~pwalls/math-python/jupyter/latex/ | https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook |",
            "url": "https://smsrikanthreddy.github.io/blog/jupyter/2021/04/17/Math-and-Jupyter-commands-in-Notebook.html",
            "relUrl": "/jupyter/2021/04/17/Math-and-Jupyter-commands-in-Notebook.html",
            "date": " • Apr 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Neural Collaborative Filtering",
            "content": "Where do we use Neural Collborative Filtering (NCF) Algorithm . In recommendation systems, mainly in ecommerce websites, we see recommendations when we search for a product. The algorithm for recommending items for a customer is chaning/improving based on customer choices, personalized recommendation etc from time to time. So to serve customers more accurately by predicting best choices of items based on customers likes, personalized views, tastes, geographical likes etc we are using Neural Collaborative Filtering . Before going throught the NCF, lets discuss the most successfull algorithm Matrix Factorization, its usecases and drawbacks. Why NCF is better at recommedning the items based on other user likes, location etc. . Matrix Factorization . Matrix factorization represents user/item as a vector of latent features which are projected into a shared feature space. In this feature space, the user-item interactions could be modeled using the inner product of user-item latent vectors. This vanilla implementation of Matrix factorization can be enhanced by integrating it with neighbor-based models, combining it with topic models of item content and extending it to factorization machines for general modeling of feature. . Equation 1 :- $$ hat{Y}_{u,i} = f(u,i|{ theta}) $$ . $y_{u,i}$: predicted score for interaction between user u and item i &lt;br&gt; ${ theta}$: model parameters &lt;br&gt; $f(Interaction Function)$: maps model parameters to the predicted score . In order to calculate theta, an objective function needs to be optimized. The 2 most popular loss functions for the recommendation system are a pointwise and pairwise loss. . Matrix factorization models the user-item interactions through a scalar product of user-item latent vectors. In mathematical terms, it is represented as follows . Equation 2 :- $$ hat{Y}_{u,i} = f(u,i|P_{u},q_{i}) = p^{T}_{u}q_{i} $ = $ sum_{k=1}^{K}p_{uk} q_{ik}$$ . $y_{u,i}$: predicted score for interaction between user u and item i $p_{u} $: latent vector for user u $q_{i}$: latent vector for item K: the dimension of latent space . Despite the effectiveness of matrix factorization for collaborative filtering, it’s performance is hindered by the simple choice of interaction function - inner product. Its performance can be improved by incorporating user-item bias terms into the interactiion function. This proves that the simple multiplication of latent features (inner product), may not be sufficient to capture the complex structure of user interaction data. . As we can see, MF models the two-way interaction of user and item latent factors, assuming each dimension of the latent space is independent of each other and linearly combining them with the same weight. As such, MF can be deemed as a linear model of latent factors . from IPython.display import Image Image(filename=&quot;../images/NCF/MF_Limitation.png&quot;) . Figure 1: An example illustrates MF’s limitation. From data matrix (a), u4 is most similar to u1, followed by u3, and lastly u2. However in the latent space (b), placing p4 closest to p1 makes p4 closer to p2 than p3 , incurring a large ranking loss. . Figure 1 illustrates how the inner product function can limit the expressiveness of MF. There are two settings to be stated clearly beforehand to understand the example well. First, since MF maps users and items to the same latent space, the similarity between two users can also be measured with an inner product, or equivalently2 , the cosine of the angle between their latent vectors. Second, without loss of generality, we use the Jaccard coefficient3 as the ground truth similarity of two users that MF needs to recover. The above figure shows the possible limitation of MF caused by the use of a simple and fixed inner product to estimate complex user–item interactions in the low-dimensional latent space. We note that one way to resolve the issue is to use a large number of latent factors K. However, it may adversely hurt the generalization of the model (e.g., overfitting the data), especially in sparse settings [26]. In this work, we address the limitation by learning the interaction function using Deep Neural Networks from data . Implicit Feedback Indirectly reflects the user’s preference through watching videos, product purchases, and clicks. The advantage of using implicit feedback is that it is easier to collect and is in surplus. The disadvantage is that there no readily available negative feedback. Explicit Feedback consists of ratings and reviews. It’s a direct feedback and the negative feedback or the likeliness of a product is readily available in terms of rating. . What is Collaborative Filtering . Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users. It works by searching a large group of people and finding a smaller set of users with tastes like a particular user. It looks at the items they like and combines them to create a ranked list of suggestions. Collaborative filtering helps in identifying features between user and item using matrix - factorization by applying inner product on the latent features of users and items. . Despite the effectiveness of matrix factorization for collaborative filtering, its performance is hindered by the simple choice of interaction function - inner product. Its performance can be improved by incorporating user-item bias terms into the interaction function. This proves that the simple multiplication of latent features (inner product), may not be sufficient to capture the complex structure of user interaction data. This calls for designing a better, dedicated interaction function for modeling the latent feature interaction between users and items. Neural Collaborative Filtering (NCF) aims to solve this by: - . Modeling user-item feature interaction through neural network architecture. It utilizes a Multi-Layer Perceptron (MLP) to learn user-item interactions. This is an upgrade over MF as MLP can (theoretically) learn any continuous function and has high level of nonlinearities (due to multiple layers) making it well-endowed to learn user-item interaction function. . | Generalizing and expressing MF as a special case of NCF. As MF is highly successful in the recommendation domain, doing this will give more credence to NCF. . | NCF Architecture . Image(filename=&quot;../images/NCF/ncf_architecture.png&quot;) . Input Layer binarise a sparse vector for a user and item identification where: Item (i): 1 means the user u has interacted with Item(i) User (u): To identify the user | Embedding layer is a fully connected layer that projects the sparse representation to a dense vector. The obtained user/item embeddings are the latent user/item vectors. | Neural CF layers use Multi-layered neural architecture to map the latent vectors to prediction scores. | The final output layer returns the predicted score by minimizing the pointwise loss/pairwise loss. | NCF modifies equation 1 in the following way: . Image(filename=&quot;../images/NCF/NCF_equation_3.png&quot;) . where P: Latent factor matrix for users (Size=M K) Q: Latent factor matrix for items (Size=N K) Theta(f): Model parameters Since f is formulated as MLP it can be expanded a . Image(filename=&quot;../images/NCF/NCF_equation_4.png&quot;) . Modeling user-item interaction . where Psi (out): mapping function for the output layer Psi (x): mapping function for the x-th neural collaborative filtering layer Equation 4 acts as the scoring function for NCF. . NCF&#8217;s loss function . Pointwise squared loss equation is represented as . Image(filename=&quot;../images/NCF/NCF_equation_5.png&quot;) . loss function here y: observed interaction in Y y negative: all/sample of unobserved interactions w(u,i): the weight of training instance (hyperparameter) . The squared loss can be explained if we assume that the observations are from a Gaussian distribution which in our case is not true. Plus the prediction score y_carat should return a score between [0,1] to represent the likelihood of the given user-item interaction. In short, we need a probabilistic approach for learning the pointwise NCF that pays special attention to the binary property of implicit data. . NCF uses a logistic /probit function at the output layer to solve for the above.With the above settings, the likelihood function is defined as : . Image(filename=&quot;../images/NCF/NCF_equation_6.png&quot;) . negative log of the likelihood function . Image(filename=&quot;../images/NCF/NCF_equation_7.png&quot;) . By employing a probabilistic treatment, NCF transforms the recommendation problem to a binary classification problem To account for negative instances y- is uniformly sampled from the unobserved interactions. . Generalized Matrix Factorization (GMF) . We now show how MF can be interpreted as a special case of our NCF framework. As MF is the most popular model for recommendation and has been investigated extensively in literature, being able to recover it allows NCF to mimic a large family of factorization models. Due to the one-hot encoding of user (item) ID of the input layer, the obtained embedding vector can be seen as the latent vector of user (item). Let the user latent vector pu be P T v U u and item latent vector qi be QT v I i . We define the mapping function of the first neural CF layer as, . Image(filename=&quot;../images/NCF/NCF_equation_8.png&quot;) . here a-out: activation function h: edge weights of the output layer We can play with a-out and h to create multiple variations of GMF. . Image(filename=&quot;../images/NCF/NCF_table.png&quot;) . As you can see from the above table that GMF with identity activation function and edge weights as 1 is indeed MF. The other 2 variations are expansions on the generic MF. The last variation of GMF with sigmoid as activation is used in NCF. NCF uses GMF with sigmoid as the activation function and learns h (the edge weights) from the data with log loss. . Multi-Layer Perceptron (MLP) . NCF is an example of multimodal deep learning as it contains data from 2 pathways namely user and item. The most intuitive way to combine them is by concatenation. But a simple vector concatenation does not account for user-item interactions and is insufficient to model the collaborative filtering effect. To address this NCF adds hidden layers on top of concatenated user-item vectors(MLP framework), to learn user-item interactions. This endows the model with a lot of flexibility and non-linearity to learn the user-item interactions. This is an upgrade over MF that uses a fixed element-wise product on them. More precisely, the MLP alter Equation 1 as follows . Image(filename=&quot;../images/NCF/NCF_equation_9.png&quot;) . where: W(x): Weight matrix b(x): bias vector a(x): activation function for the x-th layer’s perceptron p: latent vector for the user q: latent vector for an item . NCF uses ReLU as an activation function for its MLP part. Due to multiple hidden layers, the model has sufficient complexity to learn user-item interactions as compared to the fixed element-wise product of their latent vectors (MF way). . Fusion of GMF and MLP . So far we have developed two instantiations of NCF — GMF that applies a linear kernel to model the latent feature interactions, and MLP that uses a non-linear kernel to learn the interaction function from data. The question then arises: how can we fuse GMF and MLP under the NCF framework. . NCF combines these models together to superimpose their desirable characteristics. NCF concatenates the output of GMF and MLP before feeding them into NeuMF layer. . Image(filename=&quot;../images/NCF/fusion.png&quot;) . Important points to notice . GMF/MLP have separate user and item embeddings. This is to make sure that both of them learn optimal embeddings independently. | GMF replicates the vanilla MF by element-wise product of the user-item vector. | MLP takes the concatenation of user-item latent vectors as input. | The outputs of GMF and MLP are concatenated in the final NeuMF (Neural Matrix Factorization) layer. | The score function of equation 1 is modeled as, . Image(filename=&quot;../images/NCF/fusion_equation.png&quot;) . G: GMF M: MLP p: User embedding q: Item embedding . We use ReLU as the activation function of MLP layers. This model combines the linearity of MF and non-linearity of DNNs for modelling user–item latent structures. We dub this model “NeuMF”, short for Neural Matrix Factorization. . Due to the non-convex objective function of NeuMF, gradient-based optimization methods can only find locally-optimal solutions. This could be solved by good weight initializations. To solve this NCF initializes GMF and MLP with pre-trained models. There are 2 ways to do this . Random Initialization . | Train GMF+MLP with random initializations until convergence. . | Use model parameters of 1 to initialize NCF. | The weights of the two models are concatenated for the output layer as . | GMF + MLP from scratch . | Image(filename=&quot;../images/NCF/gmf_mlp.png&quot;) . where h(GMF): h vector of the pre-trained GMF h(MLP): h vector of the pre-trained MLP alpha: Hyper-parameter determining the trade-off between the 2 pre-trained models . GMF + MLP from scratch . | Adaptive Moment Estimation (Adam) adapts the learning rate for each parameter by performing smaller updates for frequent and larger updates for infrequent parameters. The Adam method yields faster convergence for both models than the vanilla SGD and relieves the pain of tuning the learning rate. . | After feeding pre-trained parameters into NeuMF, we optimize it with the vanilla SGD, rather than Adam. Adam needs to save momentum information for updating parameters. As the initialization with pre-trained networks does not store momentum information . |",
            "url": "https://smsrikanthreddy.github.io/blog/jupyter/2021/04/15/Neural-Collaborative-Filtering.html",
            "relUrl": "/jupyter/2021/04/15/Neural-Collaborative-Filtering.html",
            "date": " • Apr 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Evaluation Metrics",
            "content": "Introduction . why metrics are important . Metrics are important to evaluate models. To build better models by evaluating. | Metrics help capture business goals into quantitative target (not all errors are equal) | Metrics will help the teams effort into an definitive target/goal | Usefull to quantify the gap between, Desired performance and baseline | Desired performance and current performance | Measure progress over time | . | Useful for lower level tasks and debugging (e.g. diagnosing bias vs variance). | Ideally training objective should be the metric, but not always possible. Still, metrics are useful and important for evaluation. | . | Types of metrics There are two types of evaluation metrics, score based metrics (SVM-Regression, Linear Regression), threshold based metrics (SVM-Classification, Decision trees, Logistic Regression) | . | Different Evaluation Metrics . 1. Confusion Matrix . 2. F1 Score . 3. AUC-ROC . 4. Log Loss . 5. Gini Coefficient . 6. Root Mean Squared Error . | 1. Confusion Matrix . from IPython.display import Image Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Evaluation_Metrics/Srikanth_Confusion_Matrix_Diag.png&quot;) . Confusion Matrix is a performance measurement for machine learning classification. A confusion matrix is a NXN matrix, where N is the number of classes being predicted. For the above figure, we have N=2, and hence we get 2X2 matrix. Here are a few definitions, . TP : You have predicted positive and actual value is true | FP : You have predicted positive and actual value is false also called Type-1 Error | FN : You have predicted negative and actual value is true also called Type-2 Error | TN : You have predicted negative and actual value is false | . Accuracy : Out of all the classes, how much we predicted correctly. Total number of predictions which are correct. begin{equation*} frac{TP+TN}{TP+FP+TN+FN} end{equation*} | Precision : Out of all the positive classes we have predicted correctly, how many are actually positive begin{equation*} frac{TP}{TP+FP} end{equation*} | Recall (Sensitivity/True Positive Rate) : Out of all the true positive classes, how much we predicted correctly. It should be high as possible. begin{equation*} frac{TP}{TP+FN} end{equation*} | Specificity / True Negative Rate : Specificity tells us what proportion of the negative class got correctly classified.. It should be high as possible. begin{equation*} frac{TN}{TN+FP} end{equation*} | Fallout : Ratio of False Positive by False Positive and True Negative begin{equation*} frac{FP}{FP+TN} end{equation*} | False Negative Rate : False Negative Rate (FNR) tells us what proportion of the positive class got incorrectly classified by the classifier begin{equation*} frac{FN}{FN+TP} end{equation*} | . 2. F1 Score . It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more. begin{equation*} frac{2*Precision*Recall}{Precision+Recall} end{equation*} | . 3. AUC-ROC . The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. | AUC-ROC curve helps us visualize how well our machine learning classifier is performing. Although it works for only binary classification problems. . (Area Under the Receiver operating characteristic) | . First lets discuss about the ROC . | . Sensitivity : What proportion of positive class got correctly classified. | Specificity : what proportion of the negative class got correctly classified | . First lets try to understand ROC (Receiver operating characteristic) curve. If we look at the confusion matrix below, we observe that for a probabilistic model, we get different value for each metric. . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Evaluation_Metrics/Confusion_matrix.png&quot;) . Hence, for each sensitivity, we get a different specificity.The two vary as follows: . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Evaluation_Metrics/sensitivity_specifitity_curves.png&quot;) . The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand. . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Evaluation_Metrics/ROC.png&quot;) . Let’s take an example of threshold = 0.5 (refer to confusion matrix). Here is the confusion matrix : . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Evaluation_Metrics/Confusion_matrix2-150x129.png&quot;) . As you can see, the sensitivity at this threshold is 99.6% and the (1-specificity) is ~60%. This coordinate becomes one point in our ROC curve. To bring this curve down to a single number, we find the area under this curve (AUC). . Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb rules: . 90-1 = excellent (A) | 80-.90 = good (B) | 70-.80 = fair (C) | 60-.70 = poor (D) | 50-.60 = fail (F) | . We see that we fall under the excellent band for the current model. But this might simply be over-fitting. In such cases it becomes very important to in-time and out-of-time validations. . Points to Remember: . For a model which gives class as output, will be represented as a single point in ROC plot. . | Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared. . | In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other. . | 4. Log-Loss . AUC ROC considers the predicted probabilities for determining our model’s performance. However, there is an issue with AUC ROC, it only takes into account the order of probabilities and hence it does not take into account the model’s capability to predict higher probability for samples more likely to be positive. In that case, we could us the log loss which is nothing but negative average of the log of corrected predicted probabilities for each instance. | Logarithmic Loss (Log Loss) | Rewards confident correct answers, heavily | penalizes confident wrong answers | . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Evaluation_Metrics/log-loss.png&quot;) . p(yi) is predicted probability of positive class | 1-p(yi) is predicted probability of negative class | yi = 1 for positive class and 0 for negative class (actual values) | . References :- . http://cs229.stanford.edu/notes2020fall/notes2020fall/EvaluationMetrics.pdf | https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/ | https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234 |",
            "url": "https://smsrikanthreddy.github.io/blog/jupyter/2020/11/30/Evaluation_Metrics.html",
            "relUrl": "/jupyter/2020/11/30/Evaluation_Metrics.html",
            "date": " • Nov 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Principal Component Analysis",
            "content": "Introduction . PCA forms the basis for any ML enthusiasits. It helps in reducing the number of features by keeping almost all the data within its components . Put simply, PCA involves making a coordinate transformation (i.e. rotation) from the arbitrary axes (or &quot;features&quot;) you started with to set of axes &#39;aligned with the data itself,&#39; and doing this almost always means that you can get rid of few of these &quot;components&quot; of data that have small variance without suffering much in the way of accuracy while saving tons of computation . what we&#39;ll learn . We&#39;ll define the following terms as we go, but here&#39;s the process in a nutshell: . Covariance: Find the covariance matrix for your dataset Eigenvectors: Find the eigenvectors of that matrix (these are the &quot;components&quot; btw) Ordering: Sort the eigenvectors/&#39;dimensions&#39; from biggest to smallest variance Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space (Check: How much did we lose by that truncation?) . Covariance . If you got two data dimensions, and they vary together, then they are co-variant . STEP BY STEP EXPLANATION OF PCA . STEP 1: STANDARDIZATION . standardization is one of the important step before doing PCA or any ML problem. There could be columns whose values could be in range of 0 to 1 or 100 to 1000. so that the domination of features whose range is 100 to 1000 is more compared to 0 to 1. . The standardization is done using the below formula. . Mathematically it is done by subtracting the mean and dividing by the standard deviation for each feature. . begin{equation*} z = frac{value - mean}{standard deviation} end{equation*} Once the standardization is done, all the variables will be transformed to the same scale. . STEP 2: COVARIANCE MATRIX COMPUTATION . The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix. . The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: . from IPython.display import Image Image(filename=&quot;../images/PCA/PrincipalComponentAnalysiCovarianceMatrix.png&quot;) . Since the covariance of a variable with itself is its variance (Cov(a,a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. . What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? . It’s actually the sign of the covariance that matters : . . if positive then : the two variables increase or decrease together (correlated) . if negative then : One increases when the other decreases (Inversely correlated) . Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step . STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS . Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data . Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on . Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. . An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables . Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible . Step 4: HOW PCA CONSTRUCTS THE PRINCIPAL COMPONENTS? . The First Principal Component has the largest variance of the data. It projects most of the data points on its line. Second Principal Component is uncorelated to the first principal and perpendicular to it also and it accounts for the next highest variance and this continues. Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). This continues until a total of p principal components have been calculated, equal to the original number of variables . Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. And their number is equal to the number of dimensions of the data. For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. . Without further ado, it is eigenvectors and eigenvalues who are behind all the magic explained above, because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. . By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. . Example: . let’s suppose that our data set is 2-dimensional with 2 variables x,y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: . Image(filename=&quot;../images/PCA/eigen_vectors.png&quot;) . If we rank the eigenvalues in descending order, we get λ1&gt;λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. . After having the principal components, to compute the percentage of variance (information) accounted for by each component, we divide the eigenvalue of each component by the sum of eigenvalues. If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. . Step 5: Feature Vector . In this step we choose to keep all or some of the principal components. Generally we discard low eigen values components to create a reduced matrix of input called Feature Vector. This we call as dimensionality reduced matrix and the technique is called dimensionality reduction technique . So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions . To form the principal components for compuation, we take the transpose of the feature vector and left-multiply it with the transpose of scaled version of original dataset. . NewData = FeatureVector^T x ScaledData^T . Here, . NewData is the Matrix consisting of the principal components, . FeatureVector is the matrix we formed using the eigenvectors we chose to keep, and . ScaledData is the scaled version of original dataset . (‘T’ in the superscript denotes transpose of a matrix which is formed by interchanging the rows to columns and vice versa. In particular, a 2x3 matrix has a transpose of size 3x2) . References :- . https://builtin.com/data-science/step-step-explanation-principal-component-analysis | https://drscotthawley.github.io/PCA-From-Scratch/ | https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial |",
            "url": "https://smsrikanthreddy.github.io/blog/jupyter/2020/06/08/PCA.html",
            "relUrl": "/jupyter/2020/06/08/PCA.html",
            "date": " • Jun 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://smsrikanthreddy.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://smsrikanthreddy.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}