{
  
    
        "post0": {
            "title": "Evaluation Metrics",
            "content": "References :- . http://cs229.stanford.edu/notes2020fall/notes2020fall/EvaluationMetrics.pdf | https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/ | https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234 | Introduction . why metrics are important . Metrics are important to evaluate models. To build better models by evaluating. | Metrics help capture business goals into quantitative target (not all errors are equal) | Metrics will help the teams effort into an definitive target/goal | Usefull to quantify the gap between, Desired performance and baseline | Desired performance and current performance | Measure progress over time | . | Useful for lower level tasks and debugging (e.g. diagnosing bias vs variance). | Ideally training objective should be the metric, but not always possible. Still, metrics are useful and important for evaluation. | . | Types of metrics There are two types of evaluation metrics, score based metrics (SVM-Regression, Linear Regression), threshold based metrics (SVM-Classification, Decision trees, Logistic Regression) | . | Different Evaluation Metrics . 1. Confusion Matrix . 2. F1 Score . 3. AUC-ROC . 4. Log Loss . 5. Gini Coefficient . 6. Root Mean Squared Error . | 1. Confusion Matrix . from IPython.display import Image Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Srikanth_Confusion_Matrix_Diag.png&quot;) . Confusion Matrix is a performance measurement for machine learning classification. A confusion matrix is a NXN matrix, where N is the number of classes being predicted. For the above figure, we have N=2, and hence we get 2X2 matrix. Here are a few definitions, . TP : You have predicted positive and actual value is true | FP : You have predicted positive and actual value is false also called Type-1 Error | FN : You have predicted negative and actual value is true also called Type-2 Error | TN : You have predicted negative and actual value is false | . Accuracy : Out of all the classes, how much we predicted correctly. Total number of predictions which are correct. begin{equation*} frac{TP+TN}{TP+FP+TN+FN} end{equation*} | Precision : Out of all the positive classes we have predicted correctly, how many are actually positive begin{equation*} frac{TP}{TP+FP} end{equation*} | Recall (Sensitivity/True Positive Rate) : Out of all the true positive classes, how much we predicted correctly. It should be high as possible. begin{equation*} frac{TP}{TP+FN} end{equation*} | Specificity / True Negative Rate : Specificity tells us what proportion of the negative class got correctly classified.. It should be high as possible. begin{equation*} frac{TN}{TN+FP} end{equation*} | Fallout : Ratio of False Positive by False Positive and True Negative begin{equation*} frac{FP}{FP+TN} end{equation*} | False Negative Rate : False Negative Rate (FNR) tells us what proportion of the positive class got incorrectly classified by the classifier begin{equation*} frac{FN}{FN+TP} end{equation*} | . 2. F1 Score . It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more. begin{equation*} frac{2*Precision*Recall}{Precision+Recall} end{equation*} | . 3. AUC-ROC . The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. | AUC-ROC curve helps us visualize how well our machine learning classifier is performing. Although it works for only binary classification problems. . (Area Under the Receiver operating characteristic) | . First lets discuss about the ROC . | . Sensitivity : What proportion of positive class got correctly classified. | Specificity : what proportion of the negative class got correctly classified | . First lets try to understand ROC (Receiver operating characteristic) curve. If we look at the confusion matrix below, we observe that for a probabilistic model, we get different value for each metric. . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Confusion_matrix.png&quot;) . Hence, for each sensitivity, we get a different specificity.The two vary as follows: . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/sensitivity_specifitity_curves.png&quot;) . The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand. . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/ROC.png&quot;) . Let’s take an example of threshold = 0.5 (refer to confusion matrix). Here is the confusion matrix : . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/Confusion_matrix2-150x129.png&quot;) . As you can see, the sensitivity at this threshold is 99.6% and the (1-specificity) is ~60%. This coordinate becomes one point in our ROC curve. To bring this curve down to a single number, we find the area under this curve (AUC). . Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb rules: . 90-1 = excellent (A) | 80-.90 = good (B) | 70-.80 = fair (C) | 60-.70 = poor (D) | 50-.60 = fail (F) | . We see that we fall under the excellent band for the current model. But this might simply be over-fitting. In such cases it becomes very important to in-time and out-of-time validations. . Points to Remember: . For a model which gives class as output, will be represented as a single point in ROC plot. . | Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared. . | In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other. . | 4. Log-Loss . AUC ROC considers the predicted probabilities for determining our model’s performance. However, there is an issue with AUC ROC, it only takes into account the order of probabilities and hence it does not take into account the model’s capability to predict higher probability for samples more likely to be positive. In that case, we could us the log loss which is nothing but negative average of the log of corrected predicted probabilities for each instance. | Logarithmic Loss (Log Loss) | Rewards confident correct answers, heavily | penalizes confident wrong answers | . Image(filename=&quot;C:/Users/srmetlakunta/Documents/AI/github/blog/images/log-loss.png&quot;) . p(yi) is predicted probability of positive class | 1-p(yi) is predicted probability of negative class | yi = 1 for positive class and 0 for negative class (actual values) | .",
            "url": "https://smsrikanthreddy.github.io/blog/jupyter/2020/11/30/Evaluation_Metrics.html",
            "relUrl": "/jupyter/2020/11/30/Evaluation_Metrics.html",
            "date": " • Nov 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Principal Component Analysis",
            "content": "References :- . https://builtin.com/data-science/step-step-explanation-principal-component-analysis | https://drscotthawley.github.io/PCA-From-Scratch/ | https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial | Introduction . PCA forms the basis for any ML enthusiasits. It helps in reducing the number of features by keeping almost all the data within its components . Put simply, PCA involves making a coordinate transformation (i.e. rotation) from the arbitrary axes (or &quot;features&quot;) you started with to set of axes &#39;aligned with the data itself,&#39; and doing this almost always means that you can get rid of few of these &quot;components&quot; of data that have small variance without suffering much in the way of accuracy while saving tons of computation . what we&#39;ll learn . We&#39;ll define the following terms as we go, but here&#39;s the process in a nutshell: . Covariance: Find the covariance matrix for your dataset Eigenvectors: Find the eigenvectors of that matrix (these are the &quot;components&quot; btw) Ordering: Sort the eigenvectors/&#39;dimensions&#39; from biggest to smallest variance Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space (Check: How much did we lose by that truncation?) . Covariance . If you got two data dimensions, and they vary together, then they are co-variant . STEP BY STEP EXPLANATION OF PCA . STEP 1: STANDARDIZATION . standardization is one of the important step before doing PCA or any ML problem. There could be columns whose values could be in range of 0 to 1 or 100 to 1000. so that the domination of features whose range is 100 to 1000 is more compared to 0 to 1. . The standardization is done using the below formula. . Mathematically it is done by subtracting the mean and dividing by the standard deviation for each feature. . begin{equation*} z = frac{value - mean}{standard deviation} end{equation*} Once the standardization is done, all the variables will be transformed to the same scale. . STEP 2: COVARIANCE MATRIX COMPUTATION . The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix. . The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from: . from IPython.display import Image Image(filename=&quot;C:/Users/srmetlakunta/Desktop/PrincipalComponentAnalysiCovarianceMatrix.png&quot;) . Since the covariance of a variable with itself is its variance (Cov(a,a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal. . What do the covariances that we have as entries of the matrix tell us about the correlations between the variables? . It’s actually the sign of the covariance that matters : . . if positive then : the two variables increase or decrease together (correlated) . if negative then : One increases when the other decreases (Inversely correlated) . Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step . STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS . Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data . Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on . Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. . An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables . Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible . Step 4: HOW PCA CONSTRUCTS THE PRINCIPAL COMPONENTS? . The First Principal Component has the largest variance of the data. It projects most of the data points on its line. Second Principal Component is uncorelated to the first principal and perpendicular to it also and it accounts for the next highest variance and this continues. Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin). This continues until a total of p principal components have been calculated, equal to the original number of variables . Now that we understood what we mean by principal components, let’s go back to eigenvectors and eigenvalues. What you firstly need to know about them is that they always come in pairs, so that every eigenvector has an eigenvalue. And their number is equal to the number of dimensions of the data. For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvectors with 3 corresponding eigenvalues. . Without further ado, it is eigenvectors and eigenvalues who are behind all the magic explained above, because the eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component. . By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance. . Example: . let’s suppose that our data set is 2-dimensional with 2 variables x,y and that the eigenvectors and eigenvalues of the covariance matrix are as follows: . Image(filename=&quot;C:/Users/srmetlakunta/Desktop/eigen_vectors.png&quot;) . If we rank the eigenvalues in descending order, we get λ1&gt;λ2, which means that the eigenvector that corresponds to the first principal component (PC1) is v1 and the one that corresponds to the second component (PC2) isv2. . After having the principal components, to compute the percentage of variance (information) accounted for by each component, we divide the eigenvalue of each component by the sum of eigenvalues. If we apply this on the example above, we find that PC1 and PC2 carry respectively 96% and 4% of the variance of the data. . Step 5: Feature Vector . In this step we choose to keep all or some of the principal components. Generally we discard low eigen values components to create a reduced matrix of input called Feature Vector. This we call as dimensionality reduced matrix and the technique is called dimensionality reduction technique . So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions . To form the principal components for compuation, we take the transpose of the feature vector and left-multiply it with the transpose of scaled version of original dataset. . NewData = FeatureVector^T x ScaledData^T . Here, . NewData is the Matrix consisting of the principal components, . FeatureVector is the matrix we formed using the eigenvectors we chose to keep, and . ScaledData is the scaled version of original dataset . (‘T’ in the superscript denotes transpose of a matrix which is formed by interchanging the rows to columns and vice versa. In particular, a 2x3 matrix has a transpose of size 3x2) .",
            "url": "https://smsrikanthreddy.github.io/blog/jupyter/2020/06/08/PCA.html",
            "relUrl": "/jupyter/2020/06/08/PCA.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd #import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://smsrikanthreddy.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://smsrikanthreddy.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://smsrikanthreddy.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://smsrikanthreddy.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}