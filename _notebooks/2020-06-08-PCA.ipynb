{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference :- \n",
    "1. https://drscotthawley.github.io/PCA-From-Scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA forms the basis for any ML enthusiasits. It helps in reducing the number of features by keeping almost all the data within its components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put simply, PCA involves making a coordinate transformation (i.e. rotation) from the arbitrary axes (or \"features\")\n",
    "you started with to set of axes 'aligned with the data itself,' and doing this almost always means that you can get \n",
    "rid of few of these \"components\" of data that have small variance without suffering much in the way of accuracy while \n",
    "saving tons of computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what we'll learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the following terms as we go, but here's the process in a nutshell:\n",
    "\n",
    "Covariance: Find the covariance matrix for your dataset <br>\n",
    "Eigenvectors: Find the eigenvectors of that matrix (these are the \"components\" btw) <br>\n",
    "Ordering: Sort the eigenvectors/'dimensions' from biggest to smallest variance <br>\n",
    "Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space <br>\n",
    "(Check: How much did we lose by that truncation?)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got two data dimensions, and they vary together, then they are co-variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP BY STEP EXPLANATION OF PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 1: STANDARDIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardization is one of the important step before doing PCA or any ML problem. There could be columns whose values could be in range of 0 to 1 or 100 to 1000. so that the domination of features whose range is 100 to 1000 is more compared to 0 to 1. \n",
    "\n",
    "The standardization is done using the below formula.\n",
    "\n",
    "Mathematically it is done by subtracting the mean and dividing by the standard deviation for each feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "z = \\frac{value - mean}{standard deviation}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the standardization is done, all the variables will be transformed to the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: COVARIANCE MATRIX COMPUTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. For example, for a 3-dimensional data set with 3 variables x, y, and z, the covariance matrix is a 3×3 matrix of this from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"C:/Users/srmetlakunta/Desktop/PrincipalComponentAnalysiCovarianceMatrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the covariance of a variable with itself is its variance (Cov(a,a)=Var(a)), in the main diagonal (Top left to bottom right) we actually have the variances of each initial variable. And since the covariance is commutative (Cov(a,b)=Cov(b,a)), the entries of the covariance matrix are symmetric with respect to the main diagonal, which means that the upper and the lower triangular portions are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the covariances that we have as entries of the matrix tell us about the correlations between the variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s actually the sign of the covariance that matters :\n",
    "\n",
    ". if positive then : the two variables increase or decrease together (correlated) <br>\n",
    ". if negative then : One increases when the other decreases (Inversely correlated) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables.\n",
    "\n",
    "An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has. To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
