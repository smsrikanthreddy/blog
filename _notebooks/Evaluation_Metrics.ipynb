{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "italic-deviation",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "#### 2021-05-04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-jefferson",
   "metadata": {},
   "source": [
    "In this notebook we are going to discuss about classificatio and regression metrics. Learning these metrics help in mastering the evaluation metrics concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-oklahoma",
   "metadata": {},
   "source": [
    "```\n",
    "    1. Classification Metrics\n",
    "    2. Regression  Metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-partner",
   "metadata": {},
   "source": [
    "## 1. Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-clone",
   "metadata": {},
   "source": [
    "* There are different types of evaluation metrics in machine learning/deep learning and which metric to use depends on the dataset and also problem statement. Knowing when to use which metric is also an important job of Data Scientist.\n",
    "\n",
    "* We are going to see the following metrics in details, their mathematics etc.\n",
    "\n",
    "```\n",
    "1. Confusion Matrix\n",
    "    a. Accuracy\n",
    "    b. Precision\n",
    "    c. Recall (Sensitivity)\n",
    "    d. Specificity\n",
    "    d. F1-Score\n",
    "    \n",
    "2. AUC-ROC\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-nashville",
   "metadata": {},
   "source": [
    "### 1. Confusion Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-range",
   "metadata": {},
   "source": [
    "* Confusion matrix is the most commonly used metrics in machine learning or deep learning . The easiest way of understanding it is through diagrams. \n",
    "\n",
    "* Before diving into diagrams, let us understand what it contains and how it represented. The confusion matrix mainly used for classification problems .i.e to identify predicted classes from actual classes.\n",
    "\n",
    "* It mainly contain two outputs, one is actual outputs for all classes and the other is predicted outputs. Note that confusion matrix helps in measuring the correctness of our predicted classes from actual classes. \n",
    "\n",
    "* Let us take an example of predicted whether a person is having a cancer(1) or not (0). Note, here 1 means positive and 0 means negative\n",
    "\n",
    "||Positive|Negative|\n",
    "| :---:|:---: | :---: |\n",
    "|Positive | TP | FP|\n",
    "|Negative | FN| TN  |\n",
    "\n",
    "To understand more about confusion matrix, let us define what TP, FP etc. mean.\n",
    "\n",
    "###### True Positives (TP):- \n",
    "\n",
    "* TP mean when the predicted class output and actual classes matches i.e. when the actual output is 1 and the predicted output is also 1\n",
    "\n",
    "###### False Positives (FP):- \n",
    "\n",
    "* FP mean when the predicted class output and actual classes doesn't matches i.e. when the actual output is 0 and the predicted output is also 1 \n",
    "\n",
    "###### Flase Negatives (FN):- \n",
    "\n",
    "* FN mean when the predicted class output and actual classes doesn't matches i.e. when the actual output is 1 and the predicted output is also 0\n",
    "\n",
    "###### True Negatives (TN):- \n",
    "\n",
    "* TN mean when the predicted class output and actual classes matches i.e. when the actual output is 0 and the predicted output is also 0\n",
    "\n",
    "The ideal scenario should be that the model output 0 false positives and 0 false negatives. But that is not achievable in most of the real time scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-bargain",
   "metadata": {},
   "source": [
    "#### a). Accuracy :-\n",
    "\n",
    "* Accuracy is number of correct prediction given by the model divided by the total number of predcitions.\n",
    "Lets us see it in different way,\n",
    "\n",
    "* Accuracy =  TP + TN/ TP+FP+FN+TN\n",
    "\n",
    "* _When to use Accuracy_: \n",
    "        Accuracy is a good measure when the target variable classes in the data are nearly balanced.\n",
    "\n",
    "\n",
    "#### b). Precision :-\n",
    "* Precision is a measure that tells us what proportion of patients that we diagnosed as having cancer, actually had cancer. The predicted positives (People predicted as cancerous are TP and FP) and the people actually having a cancer are TP. Precision is defined as no. of true positives divided by the no of true positives plus no of false positivies.\n",
    "\n",
    "* Precision = TP/TP+FP\n",
    "\n",
    "\n",
    "#### c). Recall or Sensitivity :- \n",
    "* Recall is a measure that tells us what proportion of patients that actually had cancer was diagnosed by the algorithm as having cancer. The actual positives (People having cancer are TP and FN) and the people diagnosed by the model having a cancer are TP. (Note: FN is included because the Person actually had a cancer even though the model predicted otherwise.\n",
    "* The ability of a model to find all the relevant cases within a dataset. Precise definition is number of true positivies divided by the number of true positivies plus the number of false negatives.\n",
    "\n",
    "* Sensitivity = TP/TP+FN\n",
    "\n",
    "* _When to use Precision and When to use Recall?_:\n",
    "\n",
    "* It is clear that recall gives us information about a classifier’s performance with respect to false negatives (how many did we miss), while precision gives us information about its performance with respect to false positives(how many did we caught).\n",
    "\n",
    "* Precision is about being precise. So even if we managed to capture only one cancer case, and we captured it correctly, then we are 100% precise.\n",
    "\n",
    "* Recall is not so much about capturing cases correctly but more about capturing all cases that have “cancer” with the answer as “cancer”. So if we simply always say every case as “cancer”, we have 100% recall.\n",
    "\n",
    "* So basically if we want to focus more on minimising False Negatives, we would want our Recall to be as close to 100% as possible without precision being too bad and if we want to focus on minimising False positives, then our focus should be to make Precision as close to 100% as possible.\n",
    "\n",
    "#### d). Specificity :-\n",
    "* Specificity is a measure that tells us what proportion of patients that did NOT have cancer, were predicted by the model as non-cancerous. The actual negatives (People actually NOT having cancer are FP and TN) and the people diagnosed by us not having cancer are TN. (Note: FP is included because the Person did NOT actually have cancer even though the model predicted otherwise).\n",
    "\n",
    "* Specificity = TN / FP + TN\n",
    "\n",
    "* Specificity is the exact opposite of Recall\n",
    "\n",
    "#### e). F1 - Score :-\n",
    "* We don’t really want to carry both Precision and Recall in our pockets every time we make a model for solving a classification problem. So it’s best if we can get a single score that kind of represents both Precision(P) and Recall(R). F1 score is nothing but harmonic mean.\n",
    "\n",
    "* Harmonic mean is kind of an average when x and y are equal. But when x and y are different, then it’s closer to the smaller number as compared to the larger number.\n",
    "\n",
    "* While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant.\n",
    "\n",
    "* F1 Score = Harmonic Mean(Precision, Recall)\n",
    "\n",
    "* F1 Score = 2 * Precision * Recall / (Precision + Recall) \n",
    "\n",
    "* we use the harmonic mean instead of a simple average because it punishes extreme values. A classifier with precision of 1.0 and a recall of 0.0 has a simple average of 0.5 but an F1-score of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-resistance",
   "metadata": {},
   "source": [
    "So, we have seen confusion matrix with different metrics like accuracy, precision, recall, F1-score, specificity etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-sleeve",
   "metadata": {},
   "source": [
    "### 2. AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-memorial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-camping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-auckland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-caution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-hawaii",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-cologne",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recorded-shield",
   "metadata": {},
   "source": [
    "## 2. Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-envelope",
   "metadata": {},
   "source": [
    "Regression is a task when a model attempts to predict continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-belfast",
   "metadata": {},
   "source": [
    "```\n",
    "    1. Mean Absolute Error (MAE)\n",
    "    2. Mean Squared Error (MSE)\n",
    "    3. Root Mean Squared Error (RMSE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-robinson",
   "metadata": {},
   "source": [
    "### 1. Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-labor",
   "metadata": {},
   "source": [
    "* This is the mean of the absolute value or errors. \n",
    "$$ {\\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y_i})} $$\n",
    "\n",
    "* MAE wont punish large errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-underground",
   "metadata": {},
   "source": [
    "### 2. Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-treasury",
   "metadata": {},
   "source": [
    "* This is the mean of squared error. Large error are noted more than MAE, making MSE more popular\n",
    "$$ {\\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-click",
   "metadata": {},
   "source": [
    "### 3. Root Mean Squared Error(RMSE) and Root Mean Squared Log Error (RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-interface",
   "metadata": {},
   "source": [
    "RMSE and RMSLE  are used to find the difference between Actual output values(y) and predicted output values. \n",
    "\n",
    "To understand these concepts and their differences, it is important to know what does Mean Squared Error (MSE) mean. MSE incorporates both the variance and the bias of the predictor(target column). RMSE is the square root of MSE. In case of unbiased estimator, RMSE is just the square root of variance, which is actually Standard Deviation.\n",
    "\n",
    "Note: Square root of variance is standard deviation.\n",
    "\n",
    "Equations of RMSE:-\n",
    "\n",
    "$$ \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2} $$\n",
    "\n",
    "Equation of RMSLE:-\n",
    "\n",
    "$$ \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(\\log{(y_i)} - \\log{\\hat{(y_i}))}^2} $$\n",
    "\n",
    "\n",
    "\n",
    "In case of RMSLE, you take the log of the predictions and actual values. So basically, what changes is the variance that you are measuring. I believe RMSLE is usually used when you don't want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers.\n",
    "\n",
    "1. If both predicted and actual values are small: RMSE and RMSLE is same.\n",
    "2. If either predicted or the actual value is big: RMSE > RMSLE\n",
    "3. If both predicted and actual values are big: RMSE > RMSLE (RMSLE becomes almost negligible)\n",
    "\n",
    "So, RMSLE measurement is not as widely used as MSE and MAE, but it is used as the metric for the Kaggle competition that uses the bike-sharing etc. dataset. It is, effectively, the RMSE of the log-transformed predicted and target values. This measurement is useful when there is a wide range in the target variable, and you do not necessarily want to penalize large errors when the predicted and target values are themselves high. It is also effective when you care about percentage errors rather than the absolute value of errors.\n",
    "\n",
    "Lets have a look at the below example\n",
    "\n",
    "Case a) : AV = 600,  PV = 1000\n",
    "\n",
    "RMSE = 400, RMSLE = 0.5108\n",
    "\n",
    "Case b) : AV = 1400, PV = 1000\n",
    "\n",
    "RMSE = 400, RMSLE = 0.3365\n",
    "\n",
    "Here, AV = Actual Value, PV = Predicted Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-invention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hairy-southeast",
   "metadata": {},
   "source": [
    "References:-\n",
    "1. https://medium.com/greyatom/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-vanilla",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
